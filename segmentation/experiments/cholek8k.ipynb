{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this experiment I will be running multi class segmentation model in simulated VR environment. \n\nFor experiment Cholec8K dataset is used\n\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\n\n\n\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data collection","metadata":{}},{"cell_type":"code","source":"paths = []\n# Loop to collect paths to all images\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        p = os.path.join(dirname, filename)\n        paths.append(p)\nfiles = pd.DataFrame(paths) \n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:39:02.125859Z","iopub.execute_input":"2022-06-24T13:39:02.126824Z","iopub.status.idle":"2022-06-24T13:39:06.129449Z","shell.execute_reply.started":"2022-06-24T13:39:02.126775Z","shell.execute_reply":"2022-06-24T13:39:06.128454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# split paths to get masks and images\nfiles['mask_path'] = files[0].apply(lambda x: x if 'endo_mask' in x else None)\nfiles['img_path'] = files[0].apply(lambda x: x if 'endo.png' in x else None)\n\n# parse image ids\nfiles['video'] = files[0].apply(lambda x: x.split('/')[5])\nfiles['frame'] = files[0].apply(lambda x: x.split('/')[6].split('_')[1])\nfiles","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:39:06.13104Z","iopub.execute_input":"2022-06-24T13:39:06.131606Z","iopub.status.idle":"2022-06-24T13:39:06.285512Z","shell.execute_reply.started":"2022-06-24T13:39:06.13157Z","shell.execute_reply":"2022-06-24T13:39:06.284561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get rid of redundant rows\ndata = files.groupby(['video', 'frame'], as_index=False).agg({'mask_path': 'sum', 'img_path':'sum'})","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:39:07.730712Z","iopub.execute_input":"2022-06-24T13:39:07.731073Z","iopub.status.idle":"2022-06-24T13:39:07.767838Z","shell.execute_reply.started":"2022-06-24T13:39:07.731044Z","shell.execute_reply":"2022-06-24T13:39:07.766923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:39:09.193181Z","iopub.execute_input":"2022-06-24T13:39:09.193554Z","iopub.status.idle":"2022-06-24T13:39:09.209756Z","shell.execute_reply.started":"2022-06-24T13:39:09.193518Z","shell.execute_reply":"2022-06-24T13:39:09.208802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os, cv2\nimport numpy as np\nimport pandas as pd\nimport random, tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nimport albumentations as album","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:39:11.155552Z","iopub.execute_input":"2022-06-24T13:39:11.156379Z","iopub.status.idle":"2022-06-24T13:39:14.506722Z","shell.execute_reply.started":"2022-06-24T13:39:11.156341Z","shell.execute_reply":"2022-06-24T13:39:14.505692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I am using segmentation-models-pytorch for experiments\n!pip install -q -U segmentation-models-pytorch albumentations\nimport segmentation_models_pytorch as smp","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:39:14.508596Z","iopub.execute_input":"2022-06-24T13:39:14.509171Z","iopub.status.idle":"2022-06-24T13:39:36.820741Z","shell.execute_reply.started":"2022-06-24T13:39:14.509135Z","shell.execute_reply":"2022-06-24T13:39:36.819705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train test split\n\n# My train-test split is based on videos, so I may not worry about overfitting\n\ntrain_videos = ['video18',\n                 'video09',\n                 'video35',\n                 'video20',\n                 'video01',\n                 'video17',\n                 'video52',\n                 'video43',\n                 'video55',\n                 'video28',\n                'video48',\n                 'video27',\n               ]\nvalid_videos = [\n                 'video25',\n                 'video12',\n                   'video37',\n                 'video24',\n                 'video26']\n\n# For some reason the class ids are not ordered. I have to fix it\n\nids = [0, 5, 11, 12, 13, 21, 22, 23, 24, 25, 31, 32, 33, 35, 36, 50]\nreplace = {k:i for i, k in enumerate(ids)}","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:47:19.104128Z","iopub.execute_input":"2022-06-24T13:47:19.104481Z","iopub.status.idle":"2022-06-24T13:47:19.111565Z","shell.execute_reply.started":"2022-06-24T13:47:19.104449Z","shell.execute_reply":"2022-06-24T13:47:19.110515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A function to map old ids to new ones\ndef mp(entry):\n    return replace[entry] if entry in replace else entry","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:47:20.01999Z","iopub.execute_input":"2022-06-24T13:47:20.020348Z","iopub.status.idle":"2022-06-24T13:47:20.027296Z","shell.execute_reply.started":"2022-06-24T13:47:20.020315Z","shell.execute_reply":"2022-06-24T13:47:20.024847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# just vectorizing this function for robustness\nmp = np.vectorize(mp)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:47:20.718329Z","iopub.execute_input":"2022-06-24T13:47:20.71917Z","iopub.status.idle":"2022-06-24T13:47:20.724188Z","shell.execute_reply.started":"2022-06-24T13:47:20.71912Z","shell.execute_reply":"2022-06-24T13:47:20.723053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import to_categorical\n\n\nclass EndoscopyDataset(torch.utils.data.Dataset):\n    def __init__(\n            self, \n            df,\n            n_classes=16,\n            augmentation=None, \n            preprocessing=None,\n            distortion=False,\n    ):\n        self.image_paths = df['img_path'].tolist()\n        self.mask_paths = df['mask_path'].tolist()\n        self.n_classes = n_classes\n        \n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n        self.distortion = distortion\n    \n    def __getitem__(self, i):\n        \n        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n        # the mask has 3 channels, but the value is the same in all channels\n        # so using only one channel is fine\n        mask = mp(mask[:, :, 0])\n        # using to_categorical to transfer a single mask to multiple masks\n        mask = to_categorical(mask, num_classes=self.n_classes,dtype =\"float32\" if self.distortion else 'int32')\n        if self.distortion:\n            image = album.augmentations.functional.optical_distortion(image, k=4, dx=0, dy=0, interpolation=1, border_mode=0, value=None)\n            mask = album.augmentations.functional.optical_distortion(mask, k=4, dx=0, dy=0, interpolation=1, border_mode=0, value=None)\n            mask = mask.astype('int32')\n        if self.augmentation:\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n        \n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n      \n        return image, mask\n        \n    def __len__(self):\n        return len(self.image_paths)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:58:16.577542Z","iopub.execute_input":"2022-06-24T13:58:16.578601Z","iopub.status.idle":"2022-06-24T13:58:16.591694Z","shell.execute_reply.started":"2022-06-24T13:58:16.578548Z","shell.execute_reply":"2022-06-24T13:58:16.590695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_training_augmentation():\n    train_transform = [\n        album.HorizontalFlip(p=0.5),\n        album.Resize(256, 256) # it is important to resize images in this dataset\n    ]\n    return album.Compose(train_transform)\n\n\ndef get_validation_augmentation():\n    test_transform = [\n        album.Resize(256, 256) # no need to add additional augmentations\n    ]\n    return album.Compose(test_transform)\n\n\ndef to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\n\ndef get_preprocessing(preprocessing_fn=None):\n    _transform = []\n    if preprocessing_fn:\n        _transform.append(album.Lambda(image=preprocessing_fn))\n    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n        \n    return album.Compose(_transform)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:40:00.880887Z","iopub.execute_input":"2022-06-24T13:40:00.881246Z","iopub.status.idle":"2022-06-24T13:40:00.89304Z","shell.execute_reply.started":"2022-06-24T13:40:00.881214Z","shell.execute_reply":"2022-06-24T13:40:00.89185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ENCODER = 'resnet50'\nENCODER_WEIGHTS = 'imagenet'\nCLASSES = 16\nACTIVATION = 'softmax2d' # I am using a multi class segmentation, so\n                         # softmax is preferrable\n\nmodel = smp.DeepLabV3Plus(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=CLASSES, \n    activation=ACTIVATION,\n)\n\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:40:06.112706Z","iopub.execute_input":"2022-06-24T13:40:06.113101Z","iopub.status.idle":"2022-06-24T13:40:07.502833Z","shell.execute_reply.started":"2022-06-24T13:40:06.113068Z","shell.execute_reply":"2022-06-24T13:40:07.50182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data['vid'] = data['video'].apply(lambda x: x.split('_')[0]) ","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:40:07.50463Z","iopub.execute_input":"2022-06-24T13:40:07.504998Z","iopub.status.idle":"2022-06-24T13:40:07.515875Z","shell.execute_reply.started":"2022-06-24T13:40:07.504961Z","shell.execute_reply":"2022-06-24T13:40:07.514736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = EndoscopyDataset(\n    data.loc[data['vid'].isin(train_videos)], \n    augmentation=get_training_augmentation(),\n    preprocessing=get_preprocessing(preprocessing_fn),\n)\n\nvalid_dataset = EndoscopyDataset(\n    data.loc[data['vid'].isin(valid_videos)], \n    augmentation=get_validation_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn),\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:11:08.586546Z","iopub.execute_input":"2022-06-24T11:11:08.587083Z","iopub.status.idle":"2022-06-24T11:11:08.607155Z","shell.execute_reply.started":"2022-06-24T11:11:08.587035Z","shell.execute_reply":"2022-06-24T11:11:08.605515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\nvalid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:11:08.613536Z","iopub.execute_input":"2022-06-24T11:11:08.61387Z","iopub.status.idle":"2022-06-24T11:11:08.620987Z","shell.execute_reply.started":"2022-06-24T11:11:08.61384Z","shell.execute_reply":"2022-06-24T11:11:08.619408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAINING = True\n\nEPOCHS = 10\n\n# Set device: `cuda` or `cpu`\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# define loss function\nloss = smp.utils.losses.DiceLoss()\n\n# define metrics\nmetrics = [\n    smp.utils.metrics.IoU(threshold=0.5),\n    smp.utils.metrics.Fscore(threshold=0.5),\n    smp.utils.metrics.Accuracy(threshold=0.5),\n    smp.utils.metrics.Recall(threshold=0.5),\n    smp.utils.metrics.Precision(threshold=0.5),\n]\n\n\n# define optimizer\noptimizer = torch.optim.AdamW([ \n    dict(params=model.parameters(), lr=0.0001),\n])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:11:08.623288Z","iopub.execute_input":"2022-06-24T11:11:08.624347Z","iopub.status.idle":"2022-06-24T11:11:08.641508Z","shell.execute_reply.started":"2022-06-24T11:11:08.6243Z","shell.execute_reply":"2022-06-24T11:11:08.640139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_epoch = smp.utils.train.TrainEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    optimizer=optimizer,\n    device=DEVICE,\n    verbose=True,\n)\nvalid_epoch = smp.utils.train.ValidEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:11:08.643165Z","iopub.execute_input":"2022-06-24T11:11:08.643677Z","iopub.status.idle":"2022-06-24T11:11:08.698884Z","shell.execute_reply.started":"2022-06-24T11:11:08.643571Z","shell.execute_reply":"2022-06-24T11:11:08.697337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAINING:\n\n    best_iou_score = 0.0\n    train_logs_list, valid_logs_list = [], []\n\n    for i in range(0, EPOCHS):\n\n        print('\\nEpoch: {}'.format(i))\n        train_logs = train_epoch.run(train_loader)\n        valid_logs = valid_epoch.run(valid_loader)\n        train_logs_list.append(train_logs)\n        valid_logs_list.append(valid_logs)\n\n        # Save model if a better val IoU score is obtained\n        if best_iou_score < valid_logs['iou_score']:\n            best_iou_score = valid_logs['iou_score']\n            torch.save(model, './best_model.pth')\n            print('Model saved!')","metadata":{"execution":{"iopub.status.busy":"2022-06-24T11:11:08.700772Z","iopub.execute_input":"2022-06-24T11:11:08.707797Z","iopub.status.idle":"2022-06-24T12:25:34.683627Z","shell.execute_reply.started":"2022-06-24T11:11:08.707744Z","shell.execute_reply":"2022-06-24T12:25:34.682144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model with distortion\n","metadata":{}},{"cell_type":"code","source":"ENCODER = 'resnet50'\nENCODER_WEIGHTS = 'imagenet'\nCLASSES = 16\nACTIVATION = 'softmax2d' \n\nmodel = smp.DeepLabV3Plus(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=CLASSES, \n    activation=ACTIVATION,\n)\n\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:40:15.474229Z","iopub.execute_input":"2022-06-24T13:40:15.474714Z","iopub.status.idle":"2022-06-24T13:40:16.160333Z","shell.execute_reply.started":"2022-06-24T13:40:15.474672Z","shell.execute_reply":"2022-06-24T13:40:16.15919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = EndoscopyDataset(\n    data.loc[data['vid'].isin(train_videos)], \n    augmentation=get_training_augmentation(),\n    preprocessing=get_preprocessing(preprocessing_fn),\n    distortion=True, # the only difference from the previous part is this param\n)\n\nvalid_dataset = EndoscopyDataset(\n    data.loc[data['vid'].isin(valid_videos)], \n    augmentation=get_validation_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn),\n    distortion=True, # the only difference from the previous part is this param\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:58:22.728386Z","iopub.execute_input":"2022-06-24T13:58:22.729098Z","iopub.status.idle":"2022-06-24T13:58:22.743802Z","shell.execute_reply.started":"2022-06-24T13:58:22.729059Z","shell.execute_reply":"2022-06-24T13:58:22.742901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\nvalid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:58:23.27454Z","iopub.execute_input":"2022-06-24T13:58:23.27551Z","iopub.status.idle":"2022-06-24T13:58:23.281855Z","shell.execute_reply.started":"2022-06-24T13:58:23.275436Z","shell.execute_reply":"2022-06-24T13:58:23.280887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAINING = True\nEPOCHS = 10\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nloss = smp.utils.losses.DiceLoss() # uses multiclass DiceLoss by default\nmetrics = [\n    smp.utils.metrics.IoU(threshold=0.5),\n    smp.utils.metrics.Fscore(threshold=0.5),\n    smp.utils.metrics.Accuracy(threshold=0.5),\n    smp.utils.metrics.Recall(threshold=0.5),\n    smp.utils.metrics.Precision(threshold=0.5),\n]\n\noptimizer = torch.optim.Adam([ \n    dict(params=model.parameters(), lr=0.0005),\n])\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:58:24.035005Z","iopub.execute_input":"2022-06-24T13:58:24.03562Z","iopub.status.idle":"2022-06-24T13:58:24.048014Z","shell.execute_reply.started":"2022-06-24T13:58:24.035584Z","shell.execute_reply":"2022-06-24T13:58:24.046878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_epoch = smp.utils.train.TrainEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    optimizer=optimizer,\n    device=DEVICE,\n    verbose=True,\n)\nvalid_epoch = smp.utils.train.ValidEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:58:24.897445Z","iopub.execute_input":"2022-06-24T13:58:24.898216Z","iopub.status.idle":"2022-06-24T13:58:24.913268Z","shell.execute_reply.started":"2022-06-24T13:58:24.898162Z","shell.execute_reply":"2022-06-24T13:58:24.912217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAINING:\n\n    best_iou_score = 0.0\n    train_logs_list, valid_logs_list = [], []\n\n    for i in range(0, EPOCHS):\n\n        # Perform training & validation\n        print('\\nEpoch: {}'.format(i))\n        train_logs = train_epoch.run(train_loader)\n        valid_logs = valid_epoch.run(valid_loader)\n        train_logs_list.append(train_logs)\n        valid_logs_list.append(valid_logs)\n\n        # Save model if a better val IoU score is obtained\n        if best_iou_score < valid_logs['iou_score']:\n            best_iou_score = valid_logs['iou_score']\n            torch.save(model, './best_model.pth')\n            print('Model saved!')","metadata":{"execution":{"iopub.status.busy":"2022-06-24T13:58:25.672589Z","iopub.execute_input":"2022-06-24T13:58:25.673257Z","iopub.status.idle":"2022-06-24T14:51:30.392712Z","shell.execute_reply.started":"2022-06-24T13:58:25.673219Z","shell.execute_reply":"2022-06-24T14:51:30.390783Z"},"trusted":true},"execution_count":null,"outputs":[]}]}